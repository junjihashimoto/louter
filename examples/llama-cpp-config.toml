# llama-server Configuration
# ========================
# This config uses llama-server.
#

[performance]
enable_metrics = true
log_requests = true
timeout_seconds = 300

# Local gpt-oss Backend (llama.cpp server)
[backends.gpt-oss]
url = "http://localhost:11211"
protocol = "openai"  # OpenAI-compatible API
capabilities = ["text", "function_calling"]
max_tokens = 32768
temperature = 0.7
max_tokens_field = "max_tokens"  # llama.cpp uses the older field
#tool_format = "xml"  # Qwen3-Coder outputs XML format tool calls

[backends.gpt-oss.model_mapping]
"gpt-oss" = "gpt-oss:20b"
"claude-sonnet-4-5-20250929" = "gpt-oss:20b"
"claude-haiku-4-5-20251001" = "gpt-oss:20b"