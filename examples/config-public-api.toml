# Public API Configuration
# ========================
# This config uses real public APIs:
# - Google Gemini API (generativelanguage.googleapis.com)
# - OpenAI API (api.openai.com)
#
# SETUP:
# 1. Get API keys:
#    - Gemini: https://makersuite.google.com/app/apikey
#    - OpenAI: https://platform.openai.com/api-keys
#
# 2. Set environment variables:
#    export GEMINI_API_KEY="your-gemini-api-key"
#    export OPENAI_API_KEY="your-openai-api-key"
#
# 3. Run the proxy:
#    ./louter --backend openai --port 8080 --config config-public-api.toml --verbose
#
# USAGE:
# - Use Gemini models through Gemini API or OpenAI API format
# - Use OpenAI models through OpenAI API or Gemini API format
# - The proxy handles all format conversions automatically
# ========================

[performance]
enable_metrics = true
log_requests = true
timeout_seconds = 60  # Public APIs typically respond faster

# Google Gemini API Backend
[backends.gemini]
url = "https://generativelanguage.googleapis.com"
protocol = "gemini"  # Native Gemini API protocol
capabilities = ["text", "vision", "function_calling"]
priority = 1  # Higher priority for multimodal requests
max_tokens = 8192
temperature = 0.7

# API key configuration (uses GEMINI_API_KEY env var)
# Alternatively, you can set it directly here:
# api_key = "your-gemini-api-key-here"

[backends.gemini.model_mapping]
# Focus on gemini-2.5-flash (fast, multimodal, cost-effective)
"gemini-2.5-flash" = "gemini-2.5-flash"

# OpenAI API Backend
[backends.openai]
url = "https://api.openai.com"
protocol = "openai"  # Native OpenAI API protocol
capabilities = ["text", "vision", "function_calling"]
priority = 2  # Lower priority (Gemini has more capabilities)
max_tokens = 4096
# Don't set temperature - gpt-5-nano only supports default (1.0)
temperature_override = true  # Override client's temperature (with None = use default)
max_tokens_field = "max_completion_tokens"  # Newer OpenAI models require this

# API key configuration (uses OPENAI_API_KEY env var)
# Alternatively, you can set it directly here:
# api_key = "your-openai-api-key-here"

[backends.openai.model_mapping]
# Focus on gpt-5-nano (newer model, supports max_completion_tokens)
"gpt-5-nano" = "gpt-5-nano"

# Token counting mode for OpenAI backend
# Options: "estimate" (free, fast) or "chat" (accurate, uses API call)
[backends.openai.token_counting]
mode = "estimate"  # Use "chat" for accurate token counts (costs API calls)

# Local gpt-oss Backend (llama.cpp server)
[backends.gpt-oss]
url = "http://localhost:11211"
protocol = "openai"  # OpenAI-compatible API
capabilities = ["text", "function_calling"]
priority = 3  # Lowest priority (local fallback)
max_tokens = 4096
temperature = 0.7
max_tokens_field = "max_tokens"  # llama.cpp uses the older field

[backends.gpt-oss.model_mapping]
"gpt-oss" = "gpt-oss:20b"
