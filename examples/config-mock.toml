# Mock Server Configuration
# ========================
# This config points to mock servers for testing conversion logic
# without hitting real APIs.
#
# SETUP:
# 1. Start mock server:
#    cargo run --bin mock-server -- --port 8888 --data-dir test-data --verbose
#
# 2. Start proxy with mock config:
#    cargo run --bin louter -- --backend openai --port 9001 --config config-mock.toml --verbose
#
# 3. Run diagnostics:
#    cargo run --bin diagnostic -- --backend openai --port 9001 --config config-mock.toml
# ========================

[performance]
enable_metrics = true
log_requests = true
timeout_seconds = 10

# Mock OpenAI Backend
[backends.openai]
url = "http://localhost:8888"  # Mock server
protocol = "openai"
capabilities = ["text", "vision", "function_calling"]
priority = 1
max_tokens = 4096
temperature = 0.7

[backends.openai.model_mapping]
"gpt-5-nano" = "gpt-5-nano"
"gpt-4" = "gpt-4"

# Mock Gemini Backend
[backends.gemini]
url = "http://localhost:8888"  # Mock server
protocol = "gemini"
capabilities = ["text", "vision", "function_calling"]
priority = 2
max_tokens = 8192
temperature = 0.7

[backends.gemini.model_mapping]
"gemini-2.5-flash" = "gemini-2.5-flash"

# Mock Local OpenAI Backend (llama-server)
[backends.gpt-oss]
url = "http://localhost:8888"  # Mock server
protocol = "openai"
capabilities = ["text", "function_calling"]
priority = 3
max_tokens = 4096
temperature = 0.7

[backends.gpt-oss.model_mapping]
"gpt-oss" = "gpt-oss:20b"
